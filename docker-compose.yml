networks:
  esnet:
    driver: bridge

volumes:
  esdata01:
  esdata02:
  esdata03:
  airflow_postgres:
  airflow_logs:
  airflow_plugins:
  airflow_config:

x-airflow-common: &airflow-common
  image: apache/airflow:3.1.5
  user: "${AIRFLOW_UID:-50000}:0"
  environment: &airflow-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/project/dags

    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow

    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

    # Ihr Projekt muss im Container importierbar sein:
    PYTHONPATH: /opt/airflow/project

    # Elasticsearch aus Airflow-Containern erreichbar:
    ES_URL: http://es01:9200
    ES_INDEX: data-2024
    ES_ALIAS: all-data

    AIRFLOW__API_AUTH__JWT_SECRET: "${AIRFLOW__API_AUTH__JWT_SECRET}"
    AIRFLOW__API__SECRET_KEY: "${AIRFLOW__API__SECRET_KEY}"


    AIRFLOW__API__BASE_URL: "http://airflow-api-server:8080"
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-api-server:8080/execution/"

    
    AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: "airflow:admin"
    AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: "/opt/airflow/config/simple_auth_manager_passwords.json"

  volumes:
    - ./:/opt/airflow/project
    - ./airflow_runtime/logs:/opt/airflow/logs
    - ./airflow_runtime/config:/opt/airflow/config
    - ./airflow_runtime/plugins:/opt/airflow/plugins

  networks: [esnet]
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy

services:
  # -------------------------
  # Elasticsearch (3 Nodes)
  # -------------------------
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION:-8.13.4}
    container_name: es01
    environment:
      - cluster.name=${CLUSTER_NAME:-elastic-cluster}
      - node.name=es01
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_HEAP:-1g} -Xmx${ES_HEAP:-1g}
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile: { soft: 65536, hard: 65536 }
    volumes:
      - esdata01:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
    networks: [esnet]

  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION:-8.13.4}
    container_name: es02
    environment:
      - cluster.name=${CLUSTER_NAME:-elastic-cluster}
      - node.name=es02
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_HEAP:-1g} -Xmx${ES_HEAP:-1g}
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile: { soft: 65536, hard: 65536 }
    volumes:
      - esdata02:/usr/share/elasticsearch/data
    networks: [esnet]

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION:-8.13.4}
    container_name: es03
    environment:
      - cluster.name=${CLUSTER_NAME:-elastic-cluster}
      - node.name=es03
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_HEAP:-1g} -Xmx${ES_HEAP:-1g}
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile: { soft: 65536, hard: 65536 }
    volumes:
      - esdata03:/usr/share/elasticsearch/data
    networks: [esnet]

  # -------------------------
  # Airflow Dependencies
  # -------------------------
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks: [esnet]

  redis:
    image: redis:7
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks: [esnet]

  # -------------------------
  # Airflow Init (l√§uft automatisch)
  # -------------------------
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command: -c "airflow db migrate"
    restart: "no"

  # -------------------------
  # Airflow Services
  # -------------------------
  airflow-api-server:
    <<: *airflow-common
    command: airflow api-server
    ports:
      - "8080:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: airflow dag-processor
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: airflow triggerer
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: airflow celery worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
